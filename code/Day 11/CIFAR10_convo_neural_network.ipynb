{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_convo_neural_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "liAQJgBK6QsL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hHkS6zh97K-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(cifar_x_train, cifar_y_train),(cifar_x_test, cifar_y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9jxdfb_8tFv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_batch_norm(x, n_out, phase_train):\n",
        "\n",
        "\t#Step 1 : Grab the logits before they go through the non-linearity\n",
        "\t#Step 2: You have to normalize them by subtracting mean and dividing by standard deviation\n",
        "\t#Step 3: to restore representational power of the vectors, affine transform the vectors with : gamma*x+beta\n",
        "  beta_init = tf.constant_initializer(value=0.0,dtype=tf.float32)\n",
        "  gamma_init = tf.constant_initializer(value=1.0,dtype=tf.float32)\n",
        "  \n",
        "  beta = tf.get_variable(\"beta\", [n_out],initializer=beta_init)\n",
        "  gamma = tf.get_variable(\"gamma\", [n_out],initializer=gamma_init)\n",
        "  \n",
        "  batch_mean, batch_var = tf.nn.moments(x, [0,1,2],name='moments')\n",
        "  ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
        "  ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "  ema_mean, ema_var = ema.average(batch_mean),ema.average(batch_var)\n",
        "  \n",
        "  def mean_var_with_update():\n",
        "    with tf.control_dependencies([ema_apply_op]):\n",
        "      return tf.identity(batch_mean),tf.identity(batch_var)\n",
        "    \n",
        "  mean, var = tf.cond(phase_train,mean_var_with_update,lambda: (ema_mean, ema_var))\n",
        "  normed = tf.nn.batch_norm_with_global_normalization(x,mean, var, beta, gamma, 1e-3, True)\n",
        "  return normed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nMr7wLM18tVn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def layer_batch_norm(x, n_out, phase_train):\n",
        "    \n",
        "  beta_init = tf.constant_initializer(value=0.0,dtype=tf.float32)\n",
        "  gamma_init = tf.constant_initializer(value=1.0,dtype=tf.float32)\n",
        "  \n",
        "  beta = tf.get_variable(\"beta\", [n_out],initializer=beta_init)\n",
        "  gamma = tf.get_variable(\"gamma\", [n_out],initializer=gamma_init)\n",
        "  batch_mean, batch_var = tf.nn.moments(x, [0],name='moments')\n",
        "  \n",
        "  ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
        "  ema_apply_op = ema.apply([batch_mean, batch_var])\n",
        "  ema_mean, ema_var = ema.average(batch_mean),ema.average(batch_var)\n",
        "  \n",
        "  def mean_var_with_update():\n",
        "    with tf.control_dependencies([ema_apply_op]):\n",
        "      return tf.identity(batch_mean),tf.identity(batch_var)\n",
        "  \n",
        "  mean, var = tf.cond(phase_train,mean_var_with_update,lambda: (ema_mean, ema_var))\n",
        "  x_r = tf.reshape(x, [-1, 1, 1, n_out])\n",
        "  normed = tf.nn.batch_norm_with_global_normalization(x_r,mean, var, beta, gamma, 1e-3, True)\n",
        "  return tf.reshape(normed, [-1, n_out])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5T_jmDvl7aCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def layer(input, weight_shape, bias_shape, phase_train):\n",
        "  \n",
        "  weight_std = (2/weight_shape[0])**0.5\n",
        "  weight_init = tf.random_normal_initializer(stddev = weight_std)\n",
        "  bias_init = tf.constant_initializer(value=0)\n",
        "\n",
        "  W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
        "  b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
        "    \n",
        "  logits = tf.matmul(input,W)+b\n",
        "  return tf.nn.relu(layer_batch_norm(logits, weight_shape[1], phase_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aJkYrM817t2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d (input, weight_shape, bias_shape, phase_train, visualize=False):\n",
        "  \n",
        "  inp = weight_shape[0] * weight_shape[1] * weight_shape[2]\n",
        "  weight_init = tf.random_normal_initializer(stddev = (2.0/inp)**0.5)\n",
        "    \n",
        "  W = tf.get_variable(\"W\", weight_shape, initializer = weight_init)\n",
        "    \n",
        "  bias_init = tf.constant_initializer(value=0)\n",
        "  b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
        "   \n",
        "  conv_out = tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME')\n",
        "  logits = tf.nn.bias_add(conv_out,b)\n",
        "  return tf.nn.relu(conv_batch_norm(logits, weight_shape[3], phase_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkRxUHtB-Noo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_pool(input,k=2):\n",
        "  return tf.nn.max_pool(input, ksize = [1,k,k,1], strides = [1,k,k,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ija99bmq-Qfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def inference(x, keep_prob, phase_train):\n",
        "  x = tf.reshape(x,[-1,32,32,3])\n",
        "  with tf.variable_scope(\"conv_1\"):\n",
        "    conv_1 = conv2d(x,[5,5,3,64], [64], phase_train)\n",
        "    max_pool_1 = max_pool(conv_1)\n",
        "\n",
        "  with tf.variable_scope(\"conv_2\"):\n",
        "    conv_2 = conv2d(max_pool_1,[5,5,64,64],[64], phase_train)\n",
        "    max_pool_2 = max_pool(conv_2)\n",
        "  \n",
        "  with tf.variable_scope(\"fc_1\"):\n",
        "    dim = 1\n",
        "    for d in max_pool_2.get_shape()[1:].as_list():\n",
        "      dim *= d\n",
        "    pool_2_flat = tf.reshape(max_pool_2,[-1, dim])\n",
        "    fc1 = layer(pool_2_flat,[dim, 384],[384], phase_train)\n",
        "        \n",
        "    fc1_dropout = tf.nn.dropout(fc1, keep_prob)\n",
        "    \n",
        "  with tf.variable_scope(\"fc_2\"):\n",
        "    \n",
        "    fc2 = layer(fc1_dropout,[384,192],[192], phase_train)\n",
        "        \n",
        "    fc2_dropout = tf.nn.dropout(fc2, keep_prob)\n",
        "    \n",
        "    with tf.variable_scope(\"output\"):\n",
        "        output = layer(fc_2dropout, [192,10], [10])\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCyXQqIlCl1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(output,y):\n",
        "  xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=output)\n",
        "  loss = tf.reduce_mean(xentropy)\n",
        "  return loss\n",
        "\n",
        "# optimizer which will reduce the error and update gradients\n",
        "def training(cost, global_step):\n",
        "  tf.summary.scalar(\"cost\",cost)\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "  train_op = optimizer.minimize(cost,global_step=global_step)\n",
        "  return train_op\n",
        "\n",
        "#evaluation of the model\n",
        "def evaluation(output,y):\n",
        "  correct_pred = tf.equal(tf.argmax(output,1),tf.argmax(y,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QxgmEhOxC4Fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Parameters \n",
        "learning_rate = 0.01\n",
        "training_epochs = 300\n",
        "batch_size = 150\n",
        "display_step =20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kjgW99JcC6zL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "  print(cifar_x_train[0].shape)\n",
        "  x = tf.placeholder_with_default(False, shape=())\n",
        "  x = tf.cast(x,tf.float32)\n",
        "\n",
        "  #digits recognition placeholder\n",
        "  y = tf.placeholder_with_default(False, shape=())\n",
        "  y = tf.cast(y,tf.float32)\n",
        "\n",
        "  output = inference(x,1,0.5)\n",
        "  cost = loss(output,y)\n",
        "  global_step = tf.Variable(0,name=\"global_step\",trainable=False)\n",
        "\n",
        "  train_op = training(cost,global_step)\n",
        "  eval_op = evaluation(output,y)\n",
        "\n",
        "  summary_op = tf.summary.merge_all()\n",
        "\n",
        "  saver = tf.train.Saver()\n",
        "  sess = tf.Session()\n",
        "  summary_writer = tf.summary.FileWriter(\"feed_forward_logs/\",graph_def = sess.graph_def)\n",
        "\n",
        "  init_op = tf.initialize_all_variables()\n",
        "\n",
        "  sess.run(init_op)\n",
        "\n",
        "    #Training Cycle\n",
        "\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples/batch_size)\n",
        "\n",
        "    #Loop over all batches\n",
        "    for i in range(total_batch):\n",
        "      mbatch_x, mbatch_y = cifar_x_train.next_batch(batch_size), cifar_y_train.next_batch(batc_size)\n",
        "\n",
        "      feed_dict = {x: mbatch_x, y:mbatch_y}\n",
        "      sess.run(train_op, feed_dict= feed_dict)\n",
        "\n",
        "      minibatch_cost = sess.run(cost, feed_dict= feed_dict)\n",
        "      avg_cost += minibatch_cost/total_batch\n",
        "\n",
        "  print (\"Optimization Finished\")\n",
        "\n",
        "  test_feed_dict = {\n",
        "        x: cifar_x_test,\n",
        "        y: cifar_y_test\n",
        "    }\n",
        "\n",
        "  accuracy = sess.run(eval_op, feed_dict = test_feed_dict)\n",
        "\n",
        "  print (\"Testing accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}