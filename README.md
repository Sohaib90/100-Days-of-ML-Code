# 100-Days-of-ML-Code
<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day1"> Day 1 : </a> </h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day1/Data%20Pre-processing%20.ipynb"> Data Preprocessing for Machine Leanring Models</a> </h3>
<p> Things learned : </p>
<ul>
<li>Data Selection: Consider what data is available, what data is missing and what data can be removed.</li>
<li>Data Preprocessing: Organize selected data by formatting, cleaning and sampling from it. </li>
<li>Data Transformation: Transform preprocessed data ready for machine learning by engineering features using scaling, attribute decomposition and attribute aggregation.</li>
  <li> Used numpy, matplotlib, numpy and sklearn libraries to handle, visualize and manipulate data </li>
</ul>


<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%202"> Day 2 : </a> </h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%202/Support%20vector%20machine.ipynb"> Support Vector Machines: Implementation in Python </a> </h3>
<p> Things learned : </p>
<ul>
<li>What is SVM? Concepts and theory</li>
<li>Implementation in python: SVM and Kernel SVM</li>
<li>Different Kernel SVMs and comparison</li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%203"> Day 3 : </a> </h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%203/Decision-Trees-From-Scratch%20.ipynb"> Decison Trees: Implementation in Python (also sklearn implementation) </a> </h3>
<p> Things learned : (tutorial helps from machinelearningmastery) </p>
<ul>
<li>How Decision Trees work</li>
<li>Splits made on the basis of entropy or Gini Index</li>
<li>Implementation in sklearn and from scratch</li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%204"> Day 4 : </a> </h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%204"> Logistic Regression and Feed Forward network to recognize handwritten digits (Tensorflow) </a> </h3>
<p> Things learned : (Fundamentals of Deep Learning, Chapter 3) </p>
<ul>
<li>Logistic Regression on MNIST Data</li>
<li>Feed Forward Network on MNIST data and comparison</li>
<li>Tensorflow implementation : using variable scope and name scope for network</li>
</ul>

<h2> Day 5 : </h2>
<h3> Beyond Gradient Descent (Chapter 4 of Fundamentals of Deep Learning by Nikhil Budma)</h3>
<p> Things learned </p>
<ul>
<li>Challenges with Gradient Descent: Local Minima and their effect in deep learning error surfaces</li>
<li>Momentum based Optimization: keeping memory of grdients for smoother error surfaces</li>
<li>Learning Rate Adaptation: (1) Adagrad  (2) RMSProp  (3) Adam </li>
  <li> Adagrad accumaltes and adapts the global learning rate using istorical gradients </li>
  <li> RMSProp is exponentially weighted moving average of gradients: it enables us to "toss out" measurements we made a long time ago </li>
  <li> Adam is the variant of both RMSProp and AutoGrad </li>
</ul>

<h2> Day 6 : </h2>
<h3>MADL-Videos Day (Machine and Deep Learning- Videos Day)</h3>
<p> Watched Documentaries and videos relating ML and DL</p>
<ul>
<li><a href="https://www.netflix.com/watch/80190844?trackId=13752289&tctx=0%2C0%2C4a044771-5e15-4157-8f03-2e9481d9a476-103080659%2C%2C">Watched AlphaGo Documentary to learn how the game was built to be smart enough to beat the world champion</a></li>
  <li><a href="https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn?language=en#t-1173351">Watched Jeremy Howard's account on machine learning and computer vision integrated with machine learning </a></li>
  <li><a href="https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures#t-1052631"> Fei-Fei Li: How we're teaching computers to understand pictures </a></li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%207"> Day 7 : </a></h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%207/naive_bayes_classifier.ipynb">Naive Bayes Classification on diabetes dataset</a></h3>
<p> Things learned </p>
<ul>
<li>Implementation of Naive Bayes Classifier in python</li>
  <li>Class probability and attribute probability based classifier</li>
  <li>Functions for class probabilities and attribute probabilities</li>
  <li>More work required on the concepts </li>
</ul>

<h2> Day 8 : </h2>
<h3><a href="https://bloomberg.github.io/foml/#lectures">Lecture 2/3 of Bloomberg Foundation of Machine Learning: (2) Churn Prediction (3) Statistical Machine Learning</a></h3>
<p> Things learned </p>
<ul>
<li>How to think about a machine learning problem</li>
  <li>Howto think about the output and to analyse what we want to predict from the model</li>
  <li>While thinking about features and input values, think about the availability of all the data at deployment time</li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%209"> Day 9 : </a></h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%209/guess%20word.py">Speech Recognition with Python, a simple Guess word game</a></h3>
<p> Things learned </p>
<ul>
<li>How to use speech recognition library in python for speech_recognition from microphone</li>
  <li>Develop a small guessing game based on the recognized speech from microphone</li>
  <li>Theory of how it all works</li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%2010"> Day 10 : </a></h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%2010/convolutional_neural_networks.ipynb">Convolutional Neural Networks: Introduction and Implementation</a></h3>
<p> Things learned </p>
<ul>
<li>What are convolutional neural networks and what was the need? </li>
  <li>What are filters and feature maps and how convolution helps extracting features</li>
  <li>Implementation on MNIST data using Tensorflow</li>
</ul>

<h2><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%2011"> Day 11 : </a></h2>
<h3><a href="https://github.com/Sohaib90/100-Days-of-ML-Code/blob/master/code/Day%2011/CIFAR10_convo_neural_network.ipynb">Convolutional Neural Networks: Day 10 continued</a></h3>
<p> Things learned </p>
<ul>
<li>Batch normalization and how it is helpful for training </li>
  <li>How CIFAR 10 dataset is handled by using batch normalization</li>
  <li>Implementation of the network</li>
</ul>

<h2>Day 12</h2>
<h3>Chapter 6: Fundamentals of Deep Learning Book (Embedding and Representation Learning)</h3>
<p> Things learned </p>
<ul>
<li>Embedding and Representation Learning: A way to escape the curse of dimensionality </li>
  <li>Principal Component Analysis: concepts and mathematical formulation study</li>
  <li>AutoEncoders: Introduction and basic concepts</li>
</ul>

<h2> <a href="https://github.com/Sohaib90/100-Days-of-ML-Code/tree/master/code/Day%2013">Day 13</a></h2>
<h3>Chapter 6 (continued): Fundamentals of Deep Learning Book (Embedding and Representation Learning)</h3>
<p> Things learned </p>
<ul>
<li>Denoising Autoencoders: More Robust Autoencoders</li>
  <li>Introducing Sparsity in Autoencoders</li>
  <li>When context is important in representations: English language as an example and their representation learning</li>
  <li> Coded an mnist autoencoder (using dense layers) in keras </li>
</ul>

